"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[815],{4137:function(e,t,n){n.d(t,{Zo:function(){return u},kt:function(){return d}});var a=n(7294);function s(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){s(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,s=function(e,t){if(null==e)return{};var n,a,s={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(s[n]=e[n]);return s}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(s[n]=e[n])}return s}var i=a.createContext({}),c=function(e){var t=a.useContext(i),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},u=function(e){var t=c(e.components);return a.createElement(i.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var n=e.components,s=e.mdxType,r=e.originalType,i=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),h=c(n),d=s,m=h["".concat(i,".").concat(d)]||h[d]||p[d]||r;return n?a.createElement(m,o(o({ref:t},u),{},{components:n})):a.createElement(m,o({ref:t},u))}));function d(e,t){var n=arguments,s=t&&t.mdxType;if("string"==typeof e||s){var r=n.length,o=new Array(r);o[0]=h;var l={};for(var i in t)hasOwnProperty.call(t,i)&&(l[i]=t[i]);l.originalType=e,l.mdxType="string"==typeof e?e:s,o[1]=l;for(var c=2;c<r;c++)o[c]=n[c];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}h.displayName="MDXCreateElement"},5233:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return l},contentTitle:function(){return i},metadata:function(){return c},toc:function(){return u},default:function(){return h}});var a=n(7462),s=n(3366),r=(n(7294),n(4137)),o=["components"],l={title:"Deploying to AWS",sidebar_label:"Deploying to AWS",slug:"/deploy/aws",custom_edit_url:"https://github.com/linkedin/datahub/blob/master/docs/deploy/aws.md"},i="AWS setup guide",c={unversionedId:"docs/deploy/aws",id:"docs/deploy/aws",isDocsHomePage:!1,title:"Deploying to AWS",description:"The following is a set of instructions to quickstart DataHub on AWS Elastic Kubernetes Service (EKS). Note, the guide",source:"@site/genDocs/docs/deploy/aws.md",sourceDirName:"docs/deploy",slug:"/deploy/aws",permalink:"/docs/deploy/aws",editUrl:"https://github.com/linkedin/datahub/blob/master/docs/deploy/aws.md",tags:[],version:"current",frontMatter:{title:"Deploying to AWS",sidebar_label:"Deploying to AWS",slug:"/deploy/aws",custom_edit_url:"https://github.com/linkedin/datahub/blob/master/docs/deploy/aws.md"},sidebar:"overviewSidebar",previous:{title:"Upgrade Docker Image",permalink:"/docs/docker/datahub-upgrade"},next:{title:"Deploying to GCP",permalink:"/docs/deploy/gcp"}},u=[{value:"Prerequisites",id:"prerequisites",children:[],level:2},{value:"Start up a kubernetes cluster on AWS EKS",id:"start-up-a-kubernetes-cluster-on-aws-eks",children:[],level:2},{value:"Setup DataHub using Helm",id:"setup-datahub-using-helm",children:[],level:2},{value:"Expose endpoints using a load balancer",id:"expose-endpoints-using-a-load-balancer",children:[],level:2},{value:"Use AWS managed services for the storage layer",id:"use-aws-managed-services-for-the-storage-layer",children:[{value:"RDS",id:"rds",children:[],level:3},{value:"Elasticsearch Service",id:"elasticsearch-service",children:[],level:3},{value:"Managed Streaming for Apache Kafka (MSK)",id:"managed-streaming-for-apache-kafka-msk",children:[],level:3},{value:"AWS Glue Schema Registry",id:"aws-glue-schema-registry",children:[],level:3}],level:2}],p={toc:u};function h(e){var t=e.components,l=(0,s.Z)(e,o);return(0,r.kt)("wrapper",(0,a.Z)({},p,l,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"aws-setup-guide"},"AWS setup guide"),(0,r.kt)("p",null,"The following is a set of instructions to quickstart DataHub on AWS Elastic Kubernetes Service (EKS). Note, the guide\nassumes that you do not have a kubernetes cluster set up. If you are deploying DataHub to an existing cluster, please\nskip the corresponding sections."),(0,r.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,r.kt)("p",null,"This guide requires the following tools:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://kubernetes.io/docs/tasks/tools/"},"kubectl")," to manage kubernetes resources"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://helm.sh/docs/intro/install/"},"helm")," to deploy the resources based on helm charts. Note, we only support Helm\n3."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://eksctl.io/introduction/#installation"},"eksctl")," to create and manage clusters on EKS"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html"},"AWS CLI")," to manage AWS resources")),(0,r.kt)("p",null,"To use the above tools, you need to set up AWS credentials by following\nthis ",(0,r.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html"},"guide"),"."),(0,r.kt)("h2",{id:"start-up-a-kubernetes-cluster-on-aws-eks"},"Start up a kubernetes cluster on AWS EKS"),(0,r.kt)("p",null,"Let\u2019s follow this ",(0,r.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html"},"guide")," to create a new\ncluster using eksctl. Run the following command with cluster-name set to the cluster name of choice, and region set to\nthe AWS region you are operating on."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"eksctl create cluster \\\n    --name <<cluster-name>> \\\n    --region <<region>> \\\n    --with-oidc \\\n    --nodes=3\n")),(0,r.kt)("p",null,"The command will provision an EKS cluster powered by 3 EC2 m3.large nodes and provision a VPC based networking layer."),(0,r.kt)("p",null,"If you are planning to run the storage layer (MySQL, Elasticsearch, Kafka) as pods in the cluster, you need at least 3\nnodes. If you decide to use managed storage services, you can reduce the number of nodes or use m3.medium nodes to save\ncost. Refer to this ",(0,r.kt)("a",{parentName:"p",href:"https://eksctl.io/usage/creating-and-managing-clusters/"},"guide")," to further customize the cluster\nbefore provisioning."),(0,r.kt)("p",null,"Note, OIDC setup is required for following this guide when setting up the load balancer."),(0,r.kt)("p",null,"Run ",(0,r.kt)("inlineCode",{parentName:"p"},"kubectl get nodes")," to confirm that the cluster has been setup correctly. You should get results like below"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"NAME                                          STATUS   ROLES    AGE   VERSION\nip-192-168-49-49.us-west-2.compute.internal   Ready    <none>   3h    v1.18.9-eks-d1db3c\nip-192-168-64-56.us-west-2.compute.internal   Ready    <none>   3h    v1.18.9-eks-d1db3c\nip-192-168-8-126.us-west-2.compute.internal   Ready    <none>   3h    v1.18.9-eks-d1db3c\n")),(0,r.kt)("h2",{id:"setup-datahub-using-helm"},"Setup DataHub using Helm"),(0,r.kt)("p",null,"Once the kubernetes cluster has been set up, you can deploy DataHub and it\u2019s prerequisites using helm. Please follow the\nsteps in this ",(0,r.kt)("a",{parentName:"p",href:"/docs/deploy/kubernetes"},"guide")),(0,r.kt)("h2",{id:"expose-endpoints-using-a-load-balancer"},"Expose endpoints using a load balancer"),(0,r.kt)("p",null,"Now that all the pods are up and running, you need to expose the datahub-frontend end point by setting\nup ",(0,r.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/services-networking/ingress/"},"ingress"),". To do this, you need to first set up an\ningress controller. There are\nmany ",(0,r.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/"},"ingress controllers"),"  to choose\nfrom, but here, we will follow\nthis ",(0,r.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html"},"guide")," to set up the AWS\nApplication Load Balancer(ALB) Controller."),(0,r.kt)("p",null,"First, if you did not use eksctl to setup the kubernetes cluster, make sure to go through the prerequisites listed\n",(0,r.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html"},"here"),"."),(0,r.kt)("p",null,"Download the IAM policy document for allowing the controller to make calls to AWS APIs on your behalf."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"curl -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.2.0/docs/install/iam_policy.json\n")),(0,r.kt)("p",null,"Create an IAM policy based on the policy document by running the following."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"aws iam create-policy \\\n    --policy-name AWSLoadBalancerControllerIAMPolicy \\\n    --policy-document file://iam_policy.json\n")),(0,r.kt)("p",null,"Use eksctl to create a service account that allows us to attach the above policy to kubernetes pods."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"eksctl create iamserviceaccount \\\n  --cluster=<<cluster-name>> \\\n  --namespace=kube-system \\\n  --name=aws-load-balancer-controller \\\n  --attach-policy-arn=arn:aws:iam::<<account-id>>:policy/AWSLoadBalancerControllerIAMPolicy \\\n  --override-existing-serviceaccounts \\\n  --approve      \n")),(0,r.kt)("p",null,"Install the TargetGroupBinding custom resource definition by running the following."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'kubectl apply -k "github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master"\n')),(0,r.kt)("p",null,"Add the helm chart repository containing the latest version of the ALB controller."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"helm repo add eks https://aws.github.io/eks-charts\nhelm repo update\n")),(0,r.kt)("p",null,"Install the controller into the kubernetes cluster by running the following."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"helm upgrade -i aws-load-balancer-controller eks/aws-load-balancer-controller \\\n  --set clusterName=<<cluster-name>> \\\n  --set serviceAccount.create=false \\\n  --set serviceAccount.name=aws-load-balancer-controller \\\n  -n kube-system\n")),(0,r.kt)("p",null,"Verify the install completed by running ",(0,r.kt)("inlineCode",{parentName:"p"},"kubectl get deployment -n kube-system aws-load-balancer-controller"),". It should\nreturn a result like the following."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"NAME                           READY   UP-TO-DATE   AVAILABLE   AGE\naws-load-balancer-controller   2/2     2            2           142m\n")),(0,r.kt)("p",null,"Now that the controller has been set up, we can enable ingress by updating the values.yaml (or any other values.yaml\nfile used to deploy datahub). Change datahub-frontend values to the following."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'datahub-frontend:\n  enabled: true\n  image:\n    repository: linkedin/datahub-frontend-react\n    tag: "latest"\n  ingress:\n    enabled: true\n    annotations:\n      kubernetes.io/ingress.class: alb\n      alb.ingress.kubernetes.io/scheme: internet-facing\n      alb.ingress.kubernetes.io/target-type: instance\n      alb.ingress.kubernetes.io/certificate-arn: <<certificate-arn>>\n      alb.ingress.kubernetes.io/inbound-cidrs: 0.0.0.0/0\n      alb.ingress.kubernetes.io/listen-ports: \'[{"HTTP": 80}, {"HTTPS":443}]\'\n      alb.ingress.kubernetes.io/actions.ssl-redirect: \'{"Type": "redirect", "RedirectConfig": { "Protocol": "HTTPS", "Port": "443", "StatusCode": "HTTP_301"}}\'\n    hosts:\n      - host: <<host-name>>\n        redirectPaths:\n          - path: /*\n            name: ssl-redirect\n            port: use-annotation\n        paths:\n          - /*\n')),(0,r.kt)("p",null,"You need to request a certificate in the AWS Certificate Manager by following this\n",(0,r.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/acm/latest/userguide/gs-acm-request-public.html"},"guide"),", and replace certificate-arn with\nthe ARN of the new certificate. You also need to replace host-name with the hostname of choice like\ndemo.datahubproject.io."),(0,r.kt)("p",null,"After updating the yaml file, run the following to apply the updates."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"helm upgrade --install datahub datahub/datahub --values values.yaml\n")),(0,r.kt)("p",null,"Once the upgrade completes, run ",(0,r.kt)("inlineCode",{parentName:"p"},"kubectl get ingress")," to verify the ingress setup. You should see a result like the\nfollowing."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"NAME                       CLASS    HOSTS                         ADDRESS                                                                 PORTS   AGE\ndatahub-datahub-frontend   <none>   demo.datahubproject.io   k8s-default-datahubd-80b034d83e-904097062.us-west-2.elb.amazonaws.com   80      3h5m\n")),(0,r.kt)("p",null,"Note down the elb address in the address column. Add the DNS CNAME record to the host domain pointing the host-name (\nfrom above) to the elb address. DNS updates generally take a few minutes to an hour. Once that is done, you should be\nable to access datahub-frontend through the host-name."),(0,r.kt)("h2",{id:"use-aws-managed-services-for-the-storage-layer"},"Use AWS managed services for the storage layer"),(0,r.kt)("p",null,"Managing the storage services like MySQL, Elasticsearch, and Kafka as kubernetes pods requires a great deal of\nmaintenance workload. To reduce the workload, you can use managed services like AWS ",(0,r.kt)("a",{parentName:"p",href:"https://aws.amazon.com/rds"},"RDS"),",\n",(0,r.kt)("a",{parentName:"p",href:"https://aws.amazon.com/elasticsearch-service/"},"Elasticsearch Service"),", and ",(0,r.kt)("a",{parentName:"p",href:"https://aws.amazon.com/msk/"},"Managed Kafka"),"\nas the storage layer for DataHub. Support for using AWS Neptune as graph DB is coming soon."),(0,r.kt)("h3",{id:"rds"},"RDS"),(0,r.kt)("p",null,"Provision a MySQL database in AWS RDS that shares the VPC with the kubernetes cluster or has VPC peering set up between\nthe VPC of the kubernetes cluster. Once the database is provisioned, you should be able to see the following page. Take\na note of the endpoint marked by the red box."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"AWS RDS",src:n(2961).Z})),(0,r.kt)("p",null,"First, add the DB password to kubernetes by running the following."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"kubectl delete secret mysql-secrets\nkubectl create secret generic mysql-secrets --from-literal=mysql-root-password=<<password>>\n")),(0,r.kt)("p",null,"Update the sql settings under global in the values.yaml as follows."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'  sql:\n    datasource:\n      host: "<<rds-endpoint>>:3306"\n      hostForMysqlClient: "<<rds-endpoint>>"\n      port: "3306"\n      url: "jdbc:mysql://<<rds-endpoint>>:3306/datahub?verifyServerCertificate=false&useSSL=true&useUnicode=yes&characterEncoding=UTF-8"\n      driver: "com.mysql.jdbc.Driver"\n      username: "root"\n      password:\n        secretRef: mysql-secrets\n        secretKey: mysql-root-password\n')),(0,r.kt)("p",null,"Run ",(0,r.kt)("inlineCode",{parentName:"p"},"helm upgrade --install datahub datahub/datahub --values values.yaml")," to apply the changes."),(0,r.kt)("h3",{id:"elasticsearch-service"},"Elasticsearch Service"),(0,r.kt)("p",null,"Provision an elasticsearch domain running elasticsearch version 7.9 or above that shares the VPC with the kubernetes\ncluster or has VPC peering set up between the VPC of the kubernetes cluster. Once the domain is provisioned, you should\nbe able to see the following page. Take a note of the endpoint marked by the red box."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"AWS Elasticsearch Service",src:n(4798).Z})),(0,r.kt)("p",null,"Update the elasticsearch settings under global in the values.yaml as follows."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'  elasticsearch:\n    host: <<elasticsearch-endpoint>>\n    port: "443"\n    useSSL: "true"\n')),(0,r.kt)("p",null,"You can also allow communication via HTTP (without SSL) by using the settings below."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'  elasticsearch:\n    host: <<elasticsearch-endpoint>>\n    port: "80"\n')),(0,r.kt)("p",null,"If you have fine-grained access control enabled with basic authentication, first run the following to create a k8s\nsecret with the password."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"kubectl delete secret elasticsearch-secrets\nkubectl create secret generic elasticsearch-secrets --from-literal=elasticsearch-password=<<password>>\n")),(0,r.kt)("p",null,"Then use the settings below."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'  elasticsearch:\n    host: <<elasticsearch-endpoint>>\n    port: "443"\n    useSSL: "true"\n    auth:\n      username: <<username>>\n      password:\n        secretRef: elasticsearch-secrets\n        secretName: elasticsearch-password\n')),(0,r.kt)("p",null,"Lastly, you ",(0,r.kt)("strong",{parentName:"p"},"NEED")," to set the following env variable for ",(0,r.kt)("strong",{parentName:"p"},"elasticsearchSetupJob"),'. AWS Elasticsearch/Opensearch\nservice uses OpenDistro version of Elasticsearch, which does not support the "datastream" functionality. As such, we use\na different way of creating time based indices.'),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'  elasticsearchSetupJob:\n    enabled: true\n    image:\n      repository: linkedin/datahub-elasticsearch-setup\n      tag: "***"\n    extraEnvs:\n      - name: USE_AWS_ELASTICSEARCH\n        value: "true"\n')),(0,r.kt)("p",null,"Run ",(0,r.kt)("inlineCode",{parentName:"p"},"helm upgrade --install datahub datahub/datahub --values values.yaml")," to apply the changes."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Note:"),"\nIf you have a custom setup of elastic search cluster and are deploying through docker, you can modify the configurations in datahub to point to the specific ES instance -"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"If you are using ",(0,r.kt)("inlineCode",{parentName:"li"},"docker quickstart")," you can modify the hostname and port of the ES instance in docker compose quickstart files located ",(0,r.kt)("a",{parentName:"li",href:"https://github.com/linkedin/datahub/blob/master/docker/quickstart/"},"here"),".",(0,r.kt)("ol",{parentName:"li"},(0,r.kt)("li",{parentName:"ol"},"Once you have modified the quickstart recipes you can run the quickstart command using a specific docker compose file. Sample command for that is - ",(0,r.kt)("inlineCode",{parentName:"li"},"datahub docker quickstart --quickstart-compose-file docker/quickstart/docker-compose-without-neo4j.quickstart.yml")))),(0,r.kt)("li",{parentName:"ol"},"If you are not using quickstart recipes, you can modify environment variable in GMS to point to the ES instance. The env files for datahub-gms are located ",(0,r.kt)("a",{parentName:"li",href:"https://github.com/linkedin/datahub/blob/master/docker/datahub-gms/env/"},"here"),".")),(0,r.kt)("p",null,"Further, you can find a list of properties supported to work with a custom ES instance ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/linkedin/datahub/blob/master/metadata-service/factories/src/main/java/com/linkedin/gms/factory/common/ElasticsearchSSLContextFactory.java"},"here")," and ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/linkedin/datahub/blob/master/metadata-service/factories/src/main/java/com/linkedin/gms/factory/common/RestHighLevelClientFactory.java"},"here"),"."),(0,r.kt)("p",null,"A mapping between the property name used in the above two files and the name used in docker/env file can be found ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/linkedin/datahub/blob/master/metadata-service/factories/src/main/resources/application.yml"},"here"),"."),(0,r.kt)("h3",{id:"managed-streaming-for-apache-kafka-msk"},"Managed Streaming for Apache Kafka (MSK)"),(0,r.kt)("p",null,"Provision an MSK cluster that shares the VPC with the kubernetes cluster or has VPC peering set up between the VPC of\nthe kubernetes cluster. Once the domain is provisioned, click on the \u201cView client information\u201d button in the \u2018Cluster\nSummary\u201d section. You should see a page like below. Take a note of the endpoints marked by the red boxes."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"AWS MSK",src:n(5559).Z})),(0,r.kt)("p",null,"Update the kafka settings under global in the values.yaml as follows."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'kafka:\n    bootstrap:\n      server: "<<bootstrap-server endpoint>>"\n    zookeeper:\n      server:  "<<zookeeper endpoint>>"\n    schemaregistry:\n      url: "http://prerequisites-cp-schema-registry:8081"\n    partitions: 3\n    replicationFactor: 3\n')),(0,r.kt)("p",null,"Note, the number of partitions and replicationFactor should match the number of bootstrap servers. This is by default 3\nfor AWS MSK."),(0,r.kt)("p",null,"Run ",(0,r.kt)("inlineCode",{parentName:"p"},"helm upgrade --install datahub datahub/datahub --values values.yaml")," to apply the changes."),(0,r.kt)("h3",{id:"aws-glue-schema-registry"},"AWS Glue Schema Registry"),(0,r.kt)("p",null,'You can use AWS Glue schema registry instead of the kafka schema registry. To do so, first provision an AWS Glue schema\nregistry in the "Schema Registry" tab in the AWS Glue console page.'),(0,r.kt)("p",null,"Once the registry is provisioned, you can change helm chart as follows."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"kafka:\n    bootstrap:\n      ...\n    zookeeper:\n      ...\n    schemaregistry:\n      type: AWS_GLUE\n      glue:\n        region: <<AWS region of registry>>\n        registry: <<name of registry>>\n")),(0,r.kt)("p",null,"Note, it will use the name of the topic as the schema name in the registry."),(0,r.kt)("p",null,"Before you update the pods, you need to give the k8s worker nodes the correct permissions to access the schema registry."),(0,r.kt)("p",null,"The minimum permissions required looks like this"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'{\n    "Version": "2012-10-17",\n    "Statement": [\n        {\n            "Sid": "VisualEditor0",\n            "Effect": "Allow",\n            "Action": [\n                "glue:GetRegistry",\n                "glue:ListRegistries",\n                "glue:CreateSchema",\n                "glue:UpdateSchema",\n                "glue:GetSchema",\n                "glue:ListSchemas",\n                "glue:RegisterSchemaVersion",\n                "glue:GetSchemaByDefinition",\n                "glue:GetSchemaVersion",\n                "glue:GetSchemaVersionsDiff",\n                "glue:ListSchemaVersions",\n                "glue:CheckSchemaVersionValidity",\n                "glue:PutSchemaVersionMetadata",\n                "glue:QuerySchemaVersionMetadata"\n            ],\n            "Resource": [\n                "arn:aws:glue:*:795586375822:schema/*",\n                "arn:aws:glue:us-west-2:795586375822:registry/demo-shared"\n            ]\n        },\n        {\n            "Sid": "VisualEditor1",\n            "Effect": "Allow",\n            "Action": [\n                "glue:GetSchemaVersion"\n            ],\n            "Resource": [\n                "*"\n            ]\n        }\n    ]\n}\n')),(0,r.kt)("p",null,'The latter part is required to have "*" as the resource because of an issue in the AWS Glue schema registry library.\nRefer to ',(0,r.kt)("a",{parentName:"p",href:"https://github.com/awslabs/aws-glue-schema-registry/issues/68"},"this issue")," for any updates."),(0,r.kt)("p",null,"Glue currently doesn't support AWS Signature V4. As such, we cannot use service accounts to give permissions to access\nthe schema registry. The workaround is to give the above permission to the EKS worker node's IAM role. Refer\nto ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/awslabs/aws-glue-schema-registry/issues/69"},"this issue")," for any updates."),(0,r.kt)("p",null,"Run ",(0,r.kt)("inlineCode",{parentName:"p"},"helm upgrade --install datahub datahub/datahub --values values.yaml")," to apply the changes."),(0,r.kt)("p",null,'Note, you will be seeing log "Schema Version Id is null. Trying to register the schema" on every request. This log is\nmisleading, so should be ignored. Schemas are cached, so it does not register a new version on every request (aka no\nperformance issues). This has been fixed by ',(0,r.kt)("a",{parentName:"p",href:"https://github.com/awslabs/aws-glue-schema-registry/pull/64"},"this PR")," but\nthe code has not been released yet. We will update version once a new release is out."))}h.isMDXComponent=!0},4798:function(e,t,n){t.Z=n.p+"assets/images/aws-elasticsearch-938752b80fce89de4b06290e50b04e1d.png"},5559:function(e,t,n){t.Z=n.p+"assets/images/aws-msk-b31bc0807c69c546974e922a2079cb1c.png"},2961:function(e,t,n){t.Z=n.p+"assets/images/aws-rds-bf6c0fee9db2d5bac76a8f5d2699f765.png"}}]);